# Research Quality Review: Forma Revit Add-in Usability Study

**Project**: Forma Add-in for Revit Usability Testing

**Research Plan**: See `memory/research-plans/revit-addin-usability-research-plan.md`

**Review Date**: January 22, 2026

**Reviewed By**: Research Assistant Agent

**Study Dates**: April 10-11, 2024

**Research Lead**: Tamira McCoy (UX Designer, Connected Clients team)

---

## Executive Summary

**Overall Quality Rating**: ‚ö†Ô∏è **PILOT STUDY QUALITY** - Directional signals only, not conclusive

**Key Strengths**:
- Well-structured research plan with clear goals and hypothesis
- Appropriate methodology (usability testing with Figma prototype)
- Good task design aligned with research questions
- Effective use of think-aloud protocol
- Evidence of valuable insights despite limitations

**Critical Limitations**:
- **Sample size too small** (n=2 vs. recommended 5-8)
- **Figma prototype constraints** may mask or exaggerate issues
- **Limited participant diversity** (experience levels, demographics)
- **Incomplete documentation** (compliance, screener responses, missing participant)

**Recommendation**: **Conduct 3-6 additional sessions** before making product decisions. Current findings provide valuable direction for design iteration and hypothesis generation, but should not be considered conclusive.

---

## Quality Assessment Framework

This review evaluates the study across 8 dimensions:

1. **Methodology Design** - Was the right method chosen?
2. **Sample Size & Composition** - Enough participants? Right participants?
3. **Research Execution** - Did sessions run as planned?
4. **Data Quality** - Are session notes reliable and complete?
5. **Compliance & Ethics** - Were standards met?
6. **Analysis Rigor** - Was synthesis evidence-based and unbiased?
7. **Actionability** - Are findings useful for decision-making?
8. **Confidence Levels** - How certain are we about findings?

---

## 1. Methodology Design

### Rating: ‚úÖ **STRONG**

**What Worked Well**:

**Appropriate method selection**:
- Usability testing with Figma prototype is appropriate for pre-development design validation
- Research plan clearly states rationale: "Prototype testing allows us to validate designs before development"
- Task-based approach with think-aloud protocol is industry best practice
- Aligns well with research goals (validate UI, identify friction, test mental models)

**Clear research goals and hypothesis**:
- 5 specific research goals clearly defined
- Testable hypothesis: "The new Revit add-in UI will match architects' mental models when transitioning between Forma and Revit, reducing friction and enabling seamless interoperability"
- Goals directly map to tasks (e.g., Goal 1 "Project/Proposal Management" ‚Üí Task I "Load Proposal")

**Well-designed tasks**:
- 3 tasks cover complete workflow: download/load ‚Üí refresh ‚Üí convert
- Tasks build on each other logically
- Scenarios provide realistic context
- Success criteria defined for each task
- Follow-up questions probe understanding and mental models

**Good session structure**:
- Appropriate duration (45-60 minutes)
- Introduction script covers consent and expectations
- Think-aloud protocol encouraged
- Acknowledges prototype limitations upfront: "Just want to preface that this is a Figma prototype so things may not work as you expect"

**Limitations Identified**:

**Prototype constraints**:
- Figma prototype cannot provide full interactivity or real Revit feedback
- Notetaker for P02 explicitly noted: "This might be largely a limitation of the demo where she only had the option to click through pre-planned paths"
- May create confusion that wouldn't exist in functional product (e.g., preview interactions)
- May miss issues that only appear with real data or complex projects

**Missing quantitative metrics**:
- No time-on-task measurements
- No success rate tracking (beyond qualitative "completed" vs "struggled")
- No standardized usability scales (SUS, SEQ, etc.)
- Some follow-up questions requested ratings (1-5 difficulty scale) but responses not consistently documented

**Recommendations**:
- ‚úÖ Continue with prototype testing for design validation
- ‚úÖ Follow up with functional prototype or beta testing to address interaction limitations
- Consider adding quantitative metrics (time, errors, success rates) for objective data
- Document prototype limitations that may affect specific findings

---

## 2. Sample Size & Composition

### Rating: üî¥ **CRITICAL LIMITATION**

**Sample Size Analysis**:

**Planned**: 3-4 participants
**Achieved**: 2 participants (P02, P03)
**Recommended minimum**: 5-8 participants for qualitative usability testing
**Achievement rate**: 50-67% of plan; 25-40% of recommended minimum

**Impact of n=2**:

According to Nielsen Norman Group research on sample size:
- 5 users typically uncover ~85% of usability problems
- 8 users uncover ~90% of usability problems
- 2 users uncover ~48% of usability problems (estimated)

**What this means**:
- ‚ùå **Cannot claim comprehensiveness**: Likely missing 50%+ of usability issues
- ‚ùå **Cannot establish prevalence**: Don't know how common each issue is
- ‚ùå **Cannot validate hypothesis conclusively**: Too small for reliable patterns
- ‚ö†Ô∏è **Can identify major issues**: If 2/2 participants hit same problem, it's likely real
- ‚úÖ **Can generate hypotheses**: Findings are directional and worth investigating

**Confidence by Observation Frequency**:

| Observation Frequency | Participants | Confidence | Interpretation |
|----------------------|--------------|------------|----------------|
| 2/2 (100%) | P02 + P03 | üü° Medium | Likely a real issue, but may not affect 100% of users. Prioritize for validation. |
| 1/2 (50%) | P02 or P03 | üü¢ Low | Could be: (a) affects 50% of users, (b) affects small minority, (c) participant-specific. Cannot determine with n=2. |
| 0/2 (0%) | Neither | ‚ö†Ô∏è Unknown | Doesn't mean issue doesn't exist - we likely haven't found it yet. |

**Participant Composition**:

**What we know** (from session notes):
- Both have Revit experience
- Both have Forma experience
- Both appear to be architects/designers
- P02 has used existing Forma-Revit workflow via massing
- Both appear experienced (not novices)

**What we don't know** (not documented in session notes):
- Revit expertise level on 1-5 scale (asked in screener but not documented)
- Firm size
- Geographic location (names suggest Swedish, but not confirmed)
- Years of experience
- Frequency of Revit/Forma use
- Project types they work on
- How they were recruited

**Missing from sample**:
- ‚ùå Novice Revit users (screener asked about familiarity 1-5, but both participants appear experienced)
- ‚ùå Users without Forma experience
- ‚ùå Range of firm sizes (solo practitioners, small, medium, large)
- ‚ùå Geographic diversity
- ‚ùå Varying levels of Revit expertise

**Diversity concerns**:
- Narrow experience range (both experienced) may miss novice pain points
- Both familiar with existing Forma-Revit workflow creates comparison bias
- Unknown whether sample represents target user base

**Recommendations**:
- üî¥ **CRITICAL**: Recruit 3-6 additional participants to reach minimum n=5-8
- Include broader experience range (Revit familiarity 2-4 on 1-5 scale, not just 4-5)
- Include at least 1-2 Forma novices to test learnability
- Document participant characteristics from screener responses
- Consider geographic diversity if product is global
- Stratify sample by firm size if that affects usage patterns

---

## 3. Research Execution

### Rating: üü° **ADEQUATE WITH GAPS**

**What Went Well**:

**Task completion**:
- ‚úÖ Both participants completed all 3 tasks
- ‚úÖ Sessions appear to have followed planned structure
- ‚úÖ Notetakers captured observations, quotes, and context
- ‚úÖ Think-aloud protocol appears to have been used effectively

**Facilitation quality**:
- Research lead (Tamira McCoy) guided participants through tasks
- Appropriate prompting when participants struggled
- Follow-up questions asked as planned
- Participants felt comfortable providing critical feedback

**Documentation**:
- Observations captured chronologically by task
- Direct quotes included
- Notetaker interpretations flagged ("I think this may be...")
- Behavioral observations noted (not just verbal responses)

**Execution Gaps**:

**Missing participant (P01)**:
- ‚ùå Research plan lists Jesper Staahl as participant, but no session notes provided
- ‚ùå Not clear if session was conducted but not documented, or if participant didn't complete
- Impact: Reduces sample size from planned 3 to achieved 2

**Missing 4th participant**:
- ‚ö†Ô∏è Plan noted "Potential 4th participant TBD" - never conducted or documented
- Impact: Missed opportunity to reach minimum recommended sample size

**Incomplete follow-up question responses**:
- ‚ö†Ô∏è P03 was asked difficulty rating (1-5 scale) for downloading add-in but "didn't give an actual score"
- Facilitator didn't probe further or ask participant to choose a number
- Other quantitative ratings may be missing

**Screener responses not documented**:
- ‚ùå Research plan includes 5 screening questions, but responses not in session notes
- Missing context: Revit familiarity (1-5), prior Forma add-in use, magic wand improvement wish
- Impact: Can't assess if participants matched target criteria or identify recruitment issues

**Session metadata incomplete**:
- ‚ùå Recording status not documented (was session recorded? consent obtained?)
- ‚ùå Actual session duration not documented (just "~45-60 minutes" planned)
- ‚ùå Technical issues not mentioned (did prototype work smoothly?)

**Recommendations**:
- Document all planned sessions (note if participant dropped out or session cancelled)
- Probe when participants don't provide requested ratings (or remove question if not important)
- Include screener responses in session notes for context
- Create session metadata checklist: duration, recording status, technical issues, consent obtained
- Debrief after each session to capture facilitator observations

---

## 4. Data Quality

### Rating: üü° **ADEQUATE FOR DIRECTIONAL INSIGHTS**

**Strengths**:

**Rich observational notes**:
- Notetakers captured not just what participants said, but behaviors and reactions
- Quotes appear to be verbatim or close paraphrases
- Context provided for observations
- Facilitator/notetaker interpretations flagged separately from participant actions

**Evidence-based documentation**:
- Specific examples cited
- Chronological flow preserved
- Success/failure noted for each task
- Both positive moments and pain points captured

**Notetaker perspective**:
- P02's notetaker (Nicholas Ragonese) provided helpful interpretations: "I believe this is related to..." / "I am unsure how Monica would have navigated..."
- P03's notetaker (Zhou Fang) used clear formatting and captured specific suggestions

**Limitations**:

**No recordings or transcripts**:
- ‚ùå No documentation of whether sessions were recorded
- ‚ùå No transcripts provided for verbatim analysis
- Impact: Can't verify quote accuracy or review moments in detail
- Impact: Can't create video clips for stakeholder presentations
- Impact: Relies entirely on notetaker capture (subject to bias/gaps)

**Notetaker interpretation vs. participant voice**:
- Session notes include notetaker inferences that aren't always clearly separated
- Example (P02): "Monica seemed to understand the concept of being able to use the Revit project only once clicking the 'Convert to Revit Objects'. But she did not immediately understand that the preview was itself not Revit elements."
- Question: Did Monica say this, or is it notetaker inference from behavior?

**Inconsistent detail levels**:
- P02's notes provide detailed narrative and context
- P03's notes are more bullet-point format with less connective narrative
- Some sections very detailed, others sparse

**Missing quantitative data**:
- No time-on-task measurements
- No error counts
- Inconsistent difficulty ratings (P03 asked but didn't provide score)
- No standardized usability metrics

**Prototype artifact risk**:
- Some observations may be artifacts of Figma prototype limitations rather than design issues
- P02's notetaker explicitly acknowledged this: "Given Monica's familiarity I assume this is largely a limitation of the demo"
- Hard to separate prototype issues from design issues without functional testing

**Recommendations**:
- Record all future sessions (with consent) for verbatim transcription
- Create video clips of key moments for stakeholder sharing
- Establish notetaking template separating observations from interpretations
- Add quantitative metrics to supplement qualitative insights
- Test with functional prototype to isolate design issues from prototype limitations
- Consider observer triangulation (multiple notetakers) for critical sessions

---

## 5. Compliance & Ethics

### Rating: ‚ö†Ô∏è **INCOMPLETE DOCUMENTATION**

**Compliance Status**:

| Requirement | Status | Evidence | Risk |
|-------------|--------|----------|------|
| CRA Agreement | ‚ùì Unknown | Not documented in session notes | üî¥ High if external participants |
| Recording Consent | ‚ùì Unknown | Not documented; script mentions asking but no confirmation | üü° Medium |
| PII Anonymization | ‚ö†Ô∏è Partial | Real names used in session notes but participant IDs assigned | üü¢ Low (fixable) |
| Data Storage | ‚ùì Unknown | Not documented where recordings/notes stored | üü° Medium |
| Incentive Compliance | ‚ùì Unknown | Not documented if participants were compensated | üü¢ Low |

**What We Know**:

**Recording consent in script**:
- ‚úÖ Session script includes: "Is it alright if we record this session for internal sharing purposes?"
- ‚ö†Ô∏è No documentation in session notes confirming verbal consent was obtained
- ‚ö†Ô∏è No documentation of whether sessions were actually recorded

**Participant type acknowledgment**:
- Research plan includes participant handling: "Internal users (Autodesk employees/IEs): Thank via Slack" / "External users: Gift card incentive, follow-up email"
- ‚ùì Not documented whether P02 and P03 were internal or external
- Impact on compliance: External requires CRA agreement (critical), internal does not

**PII handling**:
- Session notes use real names (Monica Pereira Da Silva, Maria Johansson)
- Findings document properly uses participant IDs (P02, P03)
- ‚ö†Ô∏è Session notes need sanitization before external sharing

**CRA status**:
- Research plan notes: "CRA Status: ‚ùì Not documented in plan (assumed for internal/IE participants, unclear for external)"
- ‚ùå No confirmation in session notes whether CRAs were signed
- ‚ö†Ô∏è Critical gap if participants were external

**What We Don't Know**:

‚ùå Were participants internal (Autodesk employees) or external?
‚ùå If external, did they sign CRA agreements?
‚ùå Were sessions actually recorded?
‚ùå If recorded, was explicit consent obtained and documented?
‚ùå Where are recordings stored? (secure location required)
‚ùå How long will data be retained?
‚ùå Were participants compensated? (incentive policy compliance)

**Risk Assessment**:

**If participants were internal/IEs**:
- Risk: üü¢ Low - CRA not required, recording for internal use acceptable with consent
- Still needed: Recording consent documentation, PII anonymization for sharing

**If participants were external**:
- Risk: üî¥ High - CRA required for Autodesk Research Ops compliance
- Impact: Research Ops may flag study; findings could be invalidated
- Needed: CRA signed before sessions, recording consent, data handling procedures

**Recommendations**:

**Immediate actions**:
- üî¥ **Confirm participant type**: Were P02 and P03 internal or external?
- üî¥ **If external**: Verify CRA agreements were signed; retroactively obtain if possible
- üü° **Confirm recording consent**: Review recordings or facilitator notes to verify consent was obtained
- üü° **Sanitize session notes**: Remove real names before sharing externally (replace with P02/P03)
- üü¢ **Document data storage**: Confirm recordings are in secure, compliant location

**Future sessions**:
- Create compliance checklist completed at start of each session
- Document CRA signature date in session notes
- Document explicit recording consent (timestamp or written confirmation)
- Use participant IDs from start (never record real names if sharing beyond core team)
- Clarify data retention policy (typically 2-3 years)

**For remaining 3-6 sessions needed**:
- ‚úÖ Obtain CRA before session starts (if external participants)
- ‚úÖ Obtain explicit recording consent and document in session notes
- ‚úÖ Use only participant IDs in all documentation from day 1
- ‚úÖ Store recordings securely and document location
- ‚úÖ Complete compliance checklist for each session

---

## 6. Analysis Rigor

### Rating: ‚úÖ **STRONG** (with appropriate caveats)

**Strengths**:

**Evidence-based synthesis**:
- ‚úÖ All findings extracted directly from session notes (anti-hallucination protocol followed)
- ‚úÖ Every pain point includes verbatim quotes from participants
- ‚úÖ Frequency clearly stated (2/2 vs 1/2 participants)
- ‚úÖ No claims made beyond what's documented in notes

**Appropriate confidence levels**:
- ‚úÖ Findings flagged with confidence ratings based on frequency
- ‚úÖ Sample size limitation acknowledged throughout
- ‚úÖ Caveats about n=2 prominently displayed
- ‚úÖ Warnings against over-generalizing from small sample

**Comprehensive coverage**:
- Pain points AND positive findings captured
- Behavioral observations in addition to verbal feedback
- Themes identified across pain points
- Opportunities linked to each pain point
- Recommendations prioritized by frequency + severity

**Transparent limitations**:
- Detailed "Limitations & Caveats" section
- Sample size concerns explicitly stated
- Participant diversity gaps acknowledged
- Methodology constraints noted
- "What We Can't Conclude" section prevents overreach

**Cross-participant synthesis**:
- Patterns identified across both participants
- Divergences noted (P02 vs P03 different experiences)
- Frequency clearly communicated (affects 2/2 vs 1/2)

**Limitations**:

**Single analyst**:
- Analysis appears to be conducted by single person (research lead or agent)
- No documentation of inter-rater reliability or triangulation
- Risk of analyst bias or interpretation errors
- No validation from multiple perspectives

**No validation workshop**:
- Findings not reviewed with stakeholders or additional researchers
- No affinity mapping or collaborative synthesis session documented
- May miss alternative interpretations

**Limited quantification**:
- Frequency stated as n/2 but no other metrics
- Severity ratings (High/Medium/Low) appear subjective
- No formal severity scoring criteria documented
- Priority ratings not clearly justified with decision criteria

**Potential Figma artifact contamination**:
- Some findings may be prototype artifacts not real design issues
- Not always clear which issues would persist in functional product
- Analysis doesn't systematically separate prototype vs. design issues

**Recommendations**:

**For current analysis**:
- ‚úÖ Analysis quality is good given data available and sample size
- ‚úÖ Appropriate caveats prevent misinterpretation
- ‚úÖ Evidence-based approach ensures findings are grounded

**For future analysis**:
- Conduct collaborative synthesis workshop with research lead, designers, product managers
- Use affinity mapping or thematic analysis with multiple analysts
- Document severity scoring criteria (e.g., blocks task + affects all users = High)
- Separate prototype artifacts from design issues explicitly
- Calculate inter-rater reliability if multiple analysts code findings

**For validation**:
- Review findings with 2-3 additional researchers or domain experts
- Present to product team and incorporate their context
- Test findings against existing research or analytics data if available

---

## 7. Actionability

### Rating: ‚úÖ **STRONG**

**Strengths**:

**Clear recommendations**:
- 10 specific recommendations provided
- Prioritized by urgency (High/Medium/Low)
- Each recommendation linked to specific pain point
- Expected impact articulated for each
- Suggested owners identified (UX Design, Engineering, Product)

**Actionable opportunities**:
- Each pain point includes specific opportunity for improvement
- Opportunities are concrete, not vague ("Add preview label" not "improve preview")
- Design implications clear (e.g., use dropdown for project switching matching proposal pattern)
- Alternative solutions suggested where appropriate

**Evidence for stakeholder buy-in**:
- Verbatim quotes for each pain point
- Frequency data (2/2 vs 1/2 participants)
- Behavioral observations supplement verbal feedback
- Positive findings highlight what to preserve

**Clear next steps**:
- Follow-up research recommendations with timeline
- Specific open questions identified
- Validation strategies suggested
- Methodology recommendations for future studies

**Hypothesis validation**:
- Clear statement of hypothesis result (Partially Supported)
- Supporting and contradicting evidence listed
- Implications for product direction articulated

**Limitations Acknowledged**:

**Recommendations limited by sample size**:
- With n=2, can't confidently prioritize based on frequency
- High-priority items based on 2/2 participants may not affect majority of users
- Medium/Low priority items based on 1/2 may be more or less critical than apparent

**Prototype limitations affect recommendations**:
- Some recommendations may not be needed in functional product
- Can't distinguish which improvements are critical vs. nice-to-have without additional testing

**Missing cost/effort estimates**:
- Recommendations don't include implementation effort or cost
- No ROI assessment (impact vs. effort)
- Teams may struggle to prioritize without resource requirements

**No decision framework**:
- Doesn't provide clear go/no-go decision criteria
- Doesn't recommend whether to proceed with development or iterate designs
- Stakeholders may expect clearer direction

**Recommendations for Enhanced Actionability**:

**Add implementation context**:
- Estimate effort for each recommendation (T-shirt sizing: S/M/L)
- Add "Quick wins" section for low-effort, high-impact items
- Create 2x2 matrix: Impact vs. Effort

**Add decision support**:
- Recommend go/no-go: "Proceed with development addressing high-priority issues" or "Iterate designs before development"
- Provide confidence-based recommendations: "Must fix" vs. "Validate with more users" vs. "Monitor in beta"

**Create design specifications**:
- Translate recommendations into specific design requirements
- Create annotated wireframes showing recommended changes
- Provide acceptance criteria for each recommendation

**Link to business goals**:
- Connect recommendations to OKRs or success metrics
- Quantify potential impact where possible (e.g., "reduce time-on-task by X%")
- Show how fixes address business objectives beyond usability

**Despite limitations, findings are actionable**:
- ‚úÖ Teams can iterate designs addressing high-priority issues
- ‚úÖ Clear direction for design improvements
- ‚úÖ Specific usability issues identified with proposed solutions
- ‚úÖ Foundation for additional research to validate

---

## 8. Overall Confidence Assessment

### Rating: üü° **MEDIUM CONFIDENCE - DIRECTIONAL ONLY**

**Confidence by Finding Type**:

| Finding Category | Confidence Level | Rationale | Use Case |
|------------------|------------------|-----------|----------|
| **Issues affecting 2/2 participants** | üü° Medium | Likely real issues, but prevalence unknown. May not affect 100% of users. | Prioritize for investigation in additional sessions. Worth addressing in design iteration. |
| **Issues affecting 1/2 participants** | üü¢ Low | Could affect 50% of users, or 5%, or be participant-specific. Can't determine with n=2. | Treat as hypothesis. Monitor in additional sessions. Don't prioritize highly unless severity is extreme. |
| **Positive findings (2/2)** | üü° Medium | Likely strengths, but may not be universally appreciated. | Preserve in redesign, but validate with more users. |
| **Hypothesis validation** | üü¢ Low | "Partially supported" based on n=2 is highly uncertain. | Treat as preliminary signal only. Reserve final conclusion for n=5-8. |
| **Missing issues (0/2)** | ‚ö†Ô∏è Unknown | With n=2, likely missing ~50% of usability issues. | Expect significant new findings with additional participants. |

**Confidence-Based Recommendations**:

**High Confidence Actions** (Safe to act on now):
1. ‚úÖ **Iterate designs** addressing 2/2 participant issues (Generic label, preview state, project switching)
2. ‚úÖ **Preserve strengths** observed in 2/2 participants (refresh icon, visibility toggles, element mapping)
3. ‚úÖ **Prepare for additional testing** with 3-6 more participants
4. ‚úÖ **Generate hypotheses** for broader validation

**Medium Confidence Actions** (Investigate further):
1. ‚ö†Ô∏è **Monitor 1/2 issues** in additional sessions (templates vs views, refresh timing, download location)
2. ‚ö†Ô∏è **Prototype recommended fixes** for high-priority issues to test in next round
3. ‚ö†Ô∏è **Validate terminology** changes with quick study before full implementation

**Low Confidence Actions** (Wait for more data):
1. ‚ùå **Don't make go/no-go decisions** on development without additional research
2. ‚ùå **Don't finalize priorities** based on current frequency data
3. ‚ùå **Don't claim hypothesis is validated** (or rejected) conclusively
4. ‚ùå **Don't implement major workflow changes** without validating with n=5-8

**Decision-Making Framework**:

| Decision Type | Can Make Now? | Evidence Needed |
|--------------|---------------|-----------------|
| Iterate designs addressing 2/2 issues | ‚úÖ Yes | Current findings sufficient for design iteration |
| Proceed with development of current design | ‚ùå No | Need n=5-8 validation before development investment |
| Prioritize features/improvements | ‚ö†Ô∏è Partial | Can prioritize 2/2 issues; need more data for 1/2 issues |
| Claim hypothesis validated | ‚ùå No | Need minimum n=5 for qualitative validation |
| Make resource allocation decisions | ‚ùå No | Need cost/benefit and broader validation |
| Share findings as directional insights | ‚úÖ Yes | With appropriate caveats about sample size |
| Test prototype fixes in next round | ‚úÖ Yes | Iterative testing with small samples is valid |

**What This Study Successfully Achieved**:

Despite limitations, this study successfully:
- ‚úÖ Validated research methodology and instruments (tasks, questions, prototype worked)
- ‚úÖ Identified specific usability issues worth investigating further
- ‚úÖ Provided direction for design iteration before additional testing
- ‚úÖ Generated hypotheses about mental model mismatches
- ‚úÖ Highlighted terminology and interaction patterns to refine
- ‚úÖ Informed follow-up research planning
- ‚úÖ Validated that prototype testing approach is feasible and valuable

**What This Study Did NOT Achieve**:

This study did NOT:
- ‚ùå Provide comprehensive usability assessment (missing ~50% of issues)
- ‚ùå Validate hypothesis conclusively
- ‚ùå Establish prevalence of identified issues
- ‚ùå Enable confident go/no-go decision on development
- ‚ùå Represent full diversity of target user base
- ‚ùå Rule out Figma prototype artifacts
- ‚ùå Provide quantitative metrics (time, success rate, errors)

---

## Overall Quality Rating: Pilot Study

**Classification**: This study should be considered a **PILOT STUDY** rather than a conclusive usability evaluation.

**Pilot Study Definition**: Preliminary research to test methodology, identify initial issues, and inform full-scale study design.

**Appropriate Uses of This Research**:
1. ‚úÖ Design iteration and refinement
2. ‚úÖ Hypothesis generation for further testing
3. ‚úÖ Methodology validation (tasks, questions, prototype approach)
4. ‚úÖ Initial prioritization for design improvements (with caveats)
5. ‚úÖ Planning and scoping full-scale research
6. ‚úÖ Stakeholder alignment on design direction
7. ‚úÖ Internal team discussions and brainstorming

**Inappropriate Uses of This Research**:
1. ‚ùå Final go/no-go decision on development
2. ‚ùå Conclusive validation of hypothesis
3. ‚ùå Comprehensive usability assessment
4. ‚ùå Quantitative claims about prevalence or severity
5. ‚ùå External publication or case studies without disclaimers
6. ‚ùå Competitive benchmarking or comparisons
7. ‚ùå Resource allocation decisions without additional data

---

## Critical Recommendations

### Immediate Actions (Before Product Decisions)

**1. Conduct 3-6 Additional Sessions** üî¥ CRITICAL
- **Why**: Current n=2 is insufficient for conclusive findings
- **Target**: Reach minimum n=5-8 for qualitative usability testing
- **Focus**: Test refined designs addressing 2/2 participant issues
- **Timeline**: Before making development go/no-go decision

**2. Diversify Participant Sample** üî¥ CRITICAL
- **Why**: Current sample limited to experienced users
- **Include**: Range of Revit expertise (2-5 on familiarity scale), Forma novices, different firm sizes
- **Document**: Screener responses, participant characteristics
- **Timeline**: In additional sessions

**3. Resolve Compliance Documentation** üî¥ CRITICAL (if external participants)
- **Why**: CRA agreement required for external participants; Research Ops enforces strictly
- **Action**: Confirm participant type; verify CRA signatures; obtain retroactively if possible
- **Document**: CRA status, recording consent in session notes
- **Timeline**: Immediately

**4. Test with Functional Prototype or Beta** üü° HIGH PRIORITY
- **Why**: Separate Figma prototype artifacts from real design issues
- **Action**: Conduct follow-up testing with interactive prototype or beta product
- **Validate**: Issues like preview confusion, refresh timing with real Revit feedback
- **Timeline**: After initial design iteration, before full development

**5. Sanitize Session Notes for Sharing** üü° HIGH PRIORITY
- **Why**: Real names should not be shared beyond core team
- **Action**: Replace Monica Pereira Da Silva ‚Üí P02, Maria Johansson ‚Üí P03 in all shared documents
- **Impact**: Enables compliant sharing with stakeholders
- **Timeline**: Before sharing beyond research team

### Design Iteration Recommendations

**Safe to Implement** (Based on 2/2 participants):
1. ‚úÖ Replace "Generic" element type label with context-based categorization
2. ‚úÖ Add clear preview state indicators ("PREVIEW" label, visual treatment)
3. ‚úÖ Add project switching affordance (dropdown or back button)
4. ‚úÖ Consider real-time preview updates (if technically feasible)

**Validate Before Implementing** (Based on 1/2 participants):
1. ‚ö†Ô∏è Templates vs. views for sending to Forma (test with more users, investigate technical constraints)
2. ‚ö†Ô∏è Combining refresh into conversion action (validate workflow preference)
3. ‚ö†Ô∏è Add-in download location and branding (test discoverability)
4. ‚ö†Ô∏è Terminology changes (quick validation study with 10-15 users)

### Research Quality Improvements for Future Sessions

**Session Execution**:
- [ ] Complete all planned sessions (don't stop at 2 of 3-4)
- [ ] Document screener responses in session notes
- [ ] Probe for quantitative ratings when requested
- [ ] Create session metadata checklist

**Compliance**:
- [ ] Complete compliance checklist at start of each session
- [ ] Document CRA signature, recording consent explicitly
- [ ] Use participant IDs from start of documentation
- [ ] Store recordings securely and document location

**Data Quality**:
- [ ] Record all sessions (with consent) for transcription
- [ ] Create video clips for stakeholder sharing
- [ ] Separate observations from interpretations in notes
- [ ] Add quantitative metrics (time, errors, success rates)

**Analysis**:
- [ ] Conduct collaborative synthesis workshop
- [ ] Multiple analysts code findings (inter-rater reliability)
- [ ] Document severity scoring criteria
- [ ] Separate prototype artifacts from design issues explicitly

**Sample**:
- [ ] Reach minimum n=5-8 participants
- [ ] Diversify experience levels, firm sizes, geographies
- [ ] Document participant characteristics
- [ ] Stratify sample by key attributes

---

## Conclusion

**Summary Assessment**:

This usability study demonstrates **strong methodology and execution** given the constraints, but is **significantly limited by small sample size (n=2)** and **incomplete documentation** (compliance, screener responses).

The research successfully:
- ‚úÖ Identified specific usability issues with clear evidence
- ‚úÖ Validated research approach and instruments
- ‚úÖ Provided actionable direction for design iteration
- ‚úÖ Generated valuable hypotheses for further testing

However, the research **cannot**:
- ‚ùå Provide conclusive validation of hypothesis
- ‚ùå Support go/no-go development decisions
- ‚ùå Represent comprehensive usability assessment
- ‚ùå Establish prevalence or generalizability of findings

**Classification**: **PILOT STUDY** - Directional signals for design iteration and hypothesis generation, not conclusive research.

**Primary Recommendation**: **Conduct 3-6 additional sessions** (reaching n=5-8 total) with broader participant diversity, functional prototype, and complete compliance documentation before making product decisions.

**Value Despite Limitations**: The findings provide valuable direction for design improvements and identify specific areas requiring attention. With appropriate caveats and additional validation, this research can effectively inform product development.

**Next Steps**:
1. üî¥ **CRITICAL**: Recruit 3-6 more participants (diversify experience levels)
2. üî¥ **CRITICAL**: Resolve compliance documentation (CRA status, consent)
3. üü° **HIGH**: Iterate designs addressing 2/2 participant issues
4. üü° **HIGH**: Test refined designs with functional prototype
5. üü¢ **MEDIUM**: Validate findings with quantitative study (n=20-30)

---

**Reviewed by**: Research Assistant Agent (following Autodesk Research Ops quality standards)

**Review Date**: January 22, 2026

**Research Quality Framework**: Based on Nielsen Norman Group usability testing guidelines, Autodesk Research Ops compliance requirements, and qualitative research best practices.

---

_This quality review applies rigorous standards while acknowledging the realities of research constraints. The goal is to ensure findings are used appropriately for decision-making and that limitations are transparent to stakeholders._
