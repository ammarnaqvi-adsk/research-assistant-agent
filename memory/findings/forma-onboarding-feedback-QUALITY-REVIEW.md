# Research Quality Review: Forma Onboarding Feedback

**Study**: Forma Onboarding Feedback (P01)
**Date**: December 16, 2025
**Facilitator**: Cl√©ment Lemaire
**Reviewer**: Research Assistant Agent
**Review Date**: January 22, 2026

---

## Executive Summary

**Overall Quality**: üü° **MODERATE** - Good exploratory interview with valuable insights, but several methodological gaps limit confidence in findings.

**Key Strengths**:
- ‚úÖ Open-ended questions captured rich, unprompted feedback
- ‚úÖ Screen sharing provided concrete examples
- ‚úÖ "Magic wand" question surfaced critical pain point
- ‚úÖ Recording consent obtained

**Critical Gaps**:
- ‚ö†Ô∏è No formal research plan or hypothesis
- ‚ö†Ô∏è No screener questions - participant fit unclear
- ‚ö†Ô∏è Sample size of 1 (n=1) - cannot generalize
- ‚ö†Ô∏è CRA compliance status unclear
- ‚ö†Ô∏è Some facilitator responses may have influenced participant

**Recommendation**: Findings are valuable for hypothesis generation but should be validated with a structured study (5-8 participants, formal research plan, screener criteria) before making major product decisions.

---

## 1. Research Design Quality

### ‚ùå No Formal Research Plan

**Issue**: This was an ad-hoc exploratory session without a documented research plan.

**Missing Components**:
- No clear research goals documented upfront
- No hypothesis to validate
- No methodology selection rationale
- No success criteria defined
- No participant recruitment criteria

**Impact**:
- Unclear if P01 was the "right" participant for specific research questions
- Cannot assess if methodology matched research goals
- Difficult to know if findings answer specific business questions

**Recommendation**:
Create a formal research plan before next round of interviews. Example research goal: "Understand onboarding barriers for Revit users transitioning to Forma within their first month of use."

---

### ‚ö†Ô∏è No Screener Questions Used

**Issue**: No documented screener questions were used to qualify P01.

**What We Know About P01**:
- Works for City Council (public sector)
- 8 years Revit experience
- Used Forma 10 years ago (as Formit)
- 2 weeks of new Forma experience
- Master planning use case

**What's Unclear**:
- Was P01 recruited specifically for onboarding feedback? Or opportunistic?
- Does P01 represent target user segment?
- Are there specific user types Autodesk wanted feedback from?

**Risk**:
Without screener criteria, we don't know if P01's experience is representative of target users. For example:
- If goal was "new users," P01 has prior Forma/Formit experience (not truly new)
- If goal was "Revit users," P01 is a great match
- If goal was "private sector architects," P01 (public sector) may not match

**Recommendation**:
Define screener criteria for follow-up study. Example:
- Qualifying: Uses Revit regularly (3+ times/week), New to Forma (<1 month), Master planning or site design role
- Disqualifying: Used Forma/Formit previously, <2 years Revit experience

---

### üü¢ Appropriate Methodology (Exploratory Interview)

**Strength**: Exploratory interview with screen sharing was appropriate for this stage.

**Why This Worked**:
- Open-ended format allowed P01 to surface unprompted pain points
- Screen sharing made issues concrete (not just reported)
- 24m 36s duration was reasonable (not too long or short)
- Covered multiple topics naturally

**Good Techniques Used**:
1. **Warm-up questions** - Established context (P01's background, use case)
2. **Screen sharing walkthrough** - Grounded discussion in real work
3. **"Magic wand" question** - Surfaced #1 pain point that might not have emerged otherwise
4. **Open-ended probes** - Let P01 describe issues in their own words

**Note**: While methodology was appropriate, sample size of 1 limits confidence. 5-8 participants recommended for pattern identification.

---

## 2. Interview Script & Question Quality

### ‚úÖ Mostly Non-Leading Questions

**Overall**: Cl√©ment's questions were largely open-ended and neutral.

**Examples of Good Questions** (inferred from transcript):
- Neutral prompts that let P01 describe issues organically
- "Magic wand" framing: "What would you change if you could?"
- Allowed P01 to show their actual work via screen sharing

**No Evidence of Leading Questions** like:
- ‚ùå "Don't you think the navigation is confusing?"
- ‚ùå "How much do you love the Google Maps integration?"
- ‚ùå "This parking feature is hard to find, right?"

---

### ‚ö†Ô∏è Facilitator Responses May Have Influenced Participant

**Issue**: Cl√©ment provided confirmations and product roadmap information during the session.

**Examples**:

1. **[5:00]** - Cl√©ment: "I mean, there's a way, but it's very manual. But we are working on something that is close to what you describe."
   - **Risk**: Confirming pain point may reduce P01's sense of urgency or willingness to elaborate further
   - **Also**: Sharing roadmap ("working on something") could anchor P01's expectations

2. **[16:40-18:25]** - Cl√©ment walked through workaround for site boundary issue
   - **Risk**: Providing solutions during research may prevent understanding of workarounds users would discover naturally
   - **Note**: P01 said "that's very helpful" but we don't know if they would have found this themselves

3. **[14:49-15:00]** - Cl√©ment confirmed balcony analysis is "coming in 2026"
   - **Risk**: Sharing future features during exploratory research can influence what participants prioritize

**Impact**:
- Moderate - These responses likely did not introduce major bias
- P01's pain points appeared authentic and unprompted
- However, some follow-up exploration may have been cut short

**Best Practice**:
In exploratory research, facilitators should:
- ‚úÖ Acknowledge feedback without confirming/denying ("That's interesting, tell me more")
- ‚úÖ Probe for workarounds before offering solutions ("How did you try to solve that?")
- ‚ùå Avoid sharing roadmap during exploratory interviews (save for end if needed)

---

### üü¢ Good Use of Behavioral Probes

**Strength**: Cl√©ment allowed P01 to demonstrate issues using screen sharing.

**Evidence**:
- P01 showed actual projects they're working on
- P01 described specific use case (6 locations, 60-400 units)
- P01 walked through how they use features (space settings, analysis tools)
- P01 demonstrated where they got stuck (street parking, cycle lanes)

**Why This Matters**:
Behavioral evidence ("show me") is more reliable than self-reported claims ("I think I would...").

---

## 3. Participant Selection Quality

### ‚ùì Unclear if P01 Was "Right" Participant

**Issue**: Without research plan or screener, we can't assess participant fit.

**If Research Goal Was**: "Understand onboarding for NEW Forma users"
- **Assessment**: ‚ö†Ô∏è **Partial Fit** - P01 has prior Formit experience from 10 years ago, so not truly "new"
- **Impact**: P01 may have had muscle memory or expectations from old version

**If Research Goal Was**: "Understand Revit-to-Forma transition"
- **Assessment**: ‚úÖ **Good Fit** - P01 has 8 years Revit experience and actively uses both tools
- **Impact**: P01 perfectly represents this segment

**If Research Goal Was**: "Understand master planning use case"
- **Assessment**: ‚úÖ **Good Fit** - P01 is actively doing master planning for City Council
- **Impact**: P01's use case is relevant and current

**If Research Goal Was**: "Understand private sector architecture firms"
- **Assessment**: ‚ùå **Poor Fit** - P01 works in public sector (City Council), different constraints/priorities
- **Impact**: Public sector may have different budget, timeline, stakeholder needs than private firms

**Recommendation**:
Clarify research goals and recruit accordingly. If goal is "Revit user onboarding," P01 is excellent. If broader onboarding study, recruit mix of:
- Revit users (like P01)
- SketchUp users
- AutoCAD users
- Truly new users (no prior Forma/Formit experience)

---

## 4. Sample Size & Statistical Validity

### üî¥ Critical Limitation: N=1 (Single Participant)

**Issue**: This study has 1 participant. Qualitative research typically needs 5-8 for pattern identification.

**What We Can Conclude**:
- ‚úÖ P01's pain points are real for P01
- ‚úÖ Findings are valid for hypothesis generation
- ‚úÖ Navigation shortcuts issue is severe for this Revit user

**What We CANNOT Conclude**:
- ‚ùå How common these pain points are among Revit users
- ‚ùå Whether other Revit users have same #1 pain point
- ‚ùå If street parking/cycle lanes are priorities for other master planners
- ‚ùå If these issues cause churn at scale

**Risk of Over-Generalization**:
- The findings document says "1/1 participants (100%)" which is technically true but misleading
- 100% of 1 person is not the same as 100% of 8 people
- Product decisions based on n=1 can be risky

**Recommendation**:
- **Immediate**: Frame findings as "hypothesis to validate" not "proven insights"
- **Next Step**: Conduct follow-up study with 5-8 Revit users transitioning to Forma
- **Validation**: Run task-based usability test specifically testing navigation shortcuts issue

---

### üü° Participant Diversity: Limited

**Issue**: Single participant from one segment.

**P01 Profile**:
- Public sector (City Council)
- Ireland (European)
- Master planning use case
- 8 years Revit experience
- Returning user (used Formit 10 years ago)

**Missing Perspectives**:
- Private sector architects
- North America / Asia regions
- Other use cases (building design, landscape architecture)
- Novice users (<2 years Revit)
- Truly new users (no Forma/Formit history)

**Impact**:
Pain points identified may be more/less severe for other segments. For example:
- Navigation shortcuts might matter less for SketchUp users (no Revit muscle memory)
- Street parking might matter less for non-European users (different infrastructure standards)
- Analysis features might matter more for sustainability-focused firms

**Recommendation**:
Next study should recruit diverse participants across:
- Geography (NA, EU, APAC)
- Sector (public, private)
- Firm size (small <20, medium 20-200, large 200+)
- Experience (novice, intermediate, expert)
- Prior tools (Revit, SketchUp, AutoCAD, none)

---

## 5. Compliance & Ethics

### ‚úÖ Recording Consent Obtained

**Strength**: Session metadata confirms recording consent was obtained.

**Evidence**:
- "Recording: ‚úÖ Yes"
- "Recording Consent: ‚úÖ Obtained"

**Best Practice Met**: Explicit consent documented in session notes.

---

### ‚ùì CRA Agreement Status Unclear

**Issue**: CRA status is noted as "‚úÖ (Assumed - Autodesk research)" but not confirmed.

**Autodesk Requirement**:
All participants must sign Customer Research Agreement (CRA) before research:
- Link: https://www.autodesk.com/company/contact-us/customer-research-agreement
- Must be signed BEFORE session, not after
- Research Ops will invalidate studies without CRA

**Risk**:
- If P01 did not sign CRA, findings may not be usable for product decisions
- Autodesk governance requires CRA compliance for all research

**Recommendation**:
- **Immediate**: Confirm CRA status with Cl√©ment
- **Future**: Document CRA signature date/method in session notes (e.g., "CRA signed via DocuSign on Dec 15, 2025")

---

### ‚úÖ Participant Anonymization

**Strength**: Session notes used participant ID (P01) throughout.

**However**:
- Original transcript includes real name: "Catherine Dollard"
- Privacy note acknowledges this: "Original participant name used in this transcript... In production, use participant IDs only"

**Best Practice**:
- Session notes correctly anonymized
- Original transcript with PII should be stored separately/securely
- All findings and reports use P01 only ‚úÖ

**Compliance Met**: Anonymization protocol followed correctly.

---

### ‚úÖ No Ethical Concerns

**Assessment**: No deceptive practices, dark patterns, or ethical issues observed.

**Good Practices**:
- Transparent about purpose (onboarding feedback)
- No manipulation or misleading information
- Participant treated respectfully
- Feedback welcomed and acknowledged

---

## 6. Data Quality & Evidence

### ‚úÖ Strong Evidence Quality

**Strength**: Transcript includes timestamps, verbatim quotes, and context.

**Evidence Quality**:
- All pain points backed by direct quotes
- Timestamps allow video clip creation [MM:SS format]
- Context provided for each observation
- Behavioral evidence from screen sharing

**Anti-Hallucination Protocol Met**:
- Findings extracted directly from transcript ‚úÖ
- No inferences presented as facts ‚úÖ
- Assumptions labeled appropriately ‚úÖ

---

### üü¢ Pain Point Prioritization: Valid

**Strength**: #1 pain point (navigation shortcuts) was explicitly stated by P01.

**Evidence**:
- P01 (22:33): "But look, you asked for feedback and **that was the hardest thing actually**."
- This was in response to "magic wand" question about overall feedback
- Not inferred - P01 directly said this

**Why This Matters**:
- Prioritization based on participant's explicit statement, not researcher interpretation
- High confidence in severity rating for this finding

---

### ‚ö†Ô∏è Missing: Task Success/Failure Data

**Gap**: No structured tasks or success metrics captured.

**What's Missing**:
- Can P01 complete key tasks? (Success rate)
- How long do tasks take? (Time-on-task)
- Where exactly do users get stuck? (Error points)
- Workarounds attempted? (Partially captured)

**Why This Matters**:
- Exploratory interview captured "what's hard" (good for discovery)
- Usability testing would capture "how hard" (good for benchmarking)

**Recommendation**:
Follow-up with task-based usability testing:
- Task 1: Import a site from AutoCAD (measure time, success rate, errors)
- Task 2: Add street parking to a road (measure attempts, workarounds)
- Task 3: Pan and zoom using Revit muscle memory (observe conflicts)

---

## 7. Findings Synthesis Quality

### ‚úÖ Findings Well-Structured

**Strength**: Findings document follows template well.

**Good Practices**:
- Pain points ranked by severity (High/Medium/Low)
- Frequency tracked (1/1 participants)
- Opportunities linked to each pain point
- Recommendations prioritized and assigned to teams

---

### ‚ö†Ô∏è Confidence Levels Could Be Clearer

**Issue**: Document says findings have "High severity" but doesn't distinguish between:
- **Severity** = How bad is this for the user experiencing it? (High for P01)
- **Confidence** = How sure are we this applies to other users? (Low, because n=1)

**Example**:
- Navigation shortcuts: **High severity** (P01 said "hardest thing") + **Low confidence** (only 1 Revit user tested)
- Street parking: **High severity** (P01 blocked) + **Unknown confidence** (is this pain point common?)

**Recommendation**:
Add confidence ratings to findings:
```markdown
**Severity**: üî¥ High - P01 explicitly identified as biggest obstacle
**Confidence**: üü° Medium - Based on 1 participant, needs validation with 5-8 Revit users
```

---

## 8. Comparison to Best Practices

### Autodesk Research Ops Standards

| Standard | Status | Notes |
|----------|--------|-------|
| **Research Plan Created** | ‚ùå No | No formal plan documented |
| **Hypothesis Defined** | ‚ùå No | Exploratory, no hypothesis to test |
| **Screener Questions Used** | ‚ùå No | No screener documented |
| **5-8 Participants (Qualitative)** | ‚ùå No | Only 1 participant |
| **CRA Agreement Signed** | ‚ùì Unclear | Assumed but not confirmed |
| **Recording Consent Obtained** | ‚úÖ Yes | Documented in session notes |
| **Participant Anonymized** | ‚úÖ Yes | P01 used throughout |
| **No Leading Questions** | ‚úÖ Yes | Questions were open-ended |
| **Evidence-Based Findings** | ‚úÖ Yes | All findings backed by quotes |
| **Video Timestamps** | ‚úÖ Yes | All quotes have [MM:SS] timestamps |

**Score**: 5/10 standards fully met, 2/10 unclear, 3/10 not met

---

## 9. Impact on Findings Confidence

### What We Can Trust

**High Confidence**:
- ‚úÖ P01's pain points are authentic and specific
- ‚úÖ Navigation shortcuts were P01's biggest obstacle (explicitly stated)
- ‚úÖ Google Maps integration is valued by P01
- ‚úÖ Analysis features work well for P01's use case

**Medium Confidence**:
- üü° Revit users generally struggle with navigation differences (needs validation)
- üü° Master planners need street parking and cycle lane features (P01's use case, may vary)

**Low Confidence / Cannot Conclude**:
- ‚ùå These are the top pain points across all Forma users
- ‚ùå These issues cause churn at scale
- ‚ùå Fixing navigation shortcuts would significantly increase adoption
- ‚ùå Other user segments have same priorities

---

## 10. Recommendations for Next Steps

### Immediate Actions (Before Making Product Decisions)

**1. Validate CRA Compliance** üî¥ HIGH PRIORITY
- Confirm Catherine Dollard signed CRA before session
- Document signature date and method
- If CRA not signed, this research may not be usable

**2. Frame Findings as Hypotheses** üî¥ HIGH PRIORITY
- Update findings document to clarify these are "hypotheses to validate" not "proven insights"
- Add confidence levels to each finding
- Acknowledge n=1 limitation more prominently

**3. Create Research Plan for Follow-Up Study** üî¥ HIGH PRIORITY
- **Goal**: Validate navigation shortcuts pain point with Revit users
- **Hypothesis**: "Revit users struggle with Forma navigation during first 2 weeks of use"
- **Methodology**: Task-based usability testing + interview
- **Participants**: 5-8 Revit users, <1 month Forma experience
- **Screener**: Revit 3+times/week, new to Forma, site design/master planning role
- **Tasks**:
  - Task 1: Navigate to a specific site feature using pan/zoom
  - Task 2: Import site plan from AutoCAD
  - Task 3: Add parking to a site
- **Success Metric**: % of participants who struggle with navigation, time to adapt

---

### Research Design Improvements for Future Studies

**1. Always Create Research Plan First**
- Define goals, hypothesis, methodology BEFORE recruiting
- Document participant criteria upfront
- Set success criteria

**2. Use Screener Questions**
- Qualifying criteria (must-haves)
- Disqualifying criteria (exclusions)
- Recruit 5-8 participants for qualitative research

**3. Document Compliance Clearly**
- CRA signature date and method
- Recording consent timestamp
- Data storage plan

**4. Separate Exploratory vs. Evaluative Research**
- Exploratory (like this): Discover unknown pain points, generate hypotheses
- Evaluative: Validate specific hypotheses with structured tasks

**5. Avoid Roadmap Sharing During Exploratory Sessions**
- Save product roadmap discussion for end of session
- Avoid confirming/denying issues during discovery
- Probe for workarounds before offering solutions

---

### What This Study Is Good For

**‚úÖ Appropriate Uses**:
- Hypothesis generation ("Revit users might struggle with navigation")
- Identifying potential pain points to investigate further
- Understanding one master planner's workflow and needs
- Recruiting P01 for future beta testing (engaged, thoughtful feedback)

**‚ùå Inappropriate Uses**:
- Making major product decisions (e.g., "redesign all navigation shortcuts")
- Generalizing to all Forma users or all Revit users
- Claiming statistical significance or high confidence
- Deprioritizing other pain points not mentioned by P01

---

## 11. Final Assessment

### Overall Quality Rating: üü° MODERATE

**Why Not Higher**:
- No formal research plan or hypothesis
- No screener questions (participant fit unclear)
- Sample size of 1 (cannot identify patterns)
- CRA compliance status unclear
- Some facilitator responses may have influenced findings

**Why Not Lower**:
- Good interview techniques (open-ended, screen sharing, magic wand)
- High-quality evidence (timestamps, quotes, context)
- No leading questions observed
- Appropriate methodology for exploratory research
- Valuable insights for hypothesis generation

---

### Key Takeaway

**This was a good exploratory interview that surfaced valuable hypotheses.** However, findings should be validated with a structured study (5-8 participants, formal research plan, screener criteria) before making significant product decisions.

**P01's feedback is a starting point, not a conclusion.**

---

### Positive Note

Despite methodological gaps, this session produced actionable insights:
- Navigation shortcuts hypothesis is specific and testable
- P01 is an engaged participant (consider for beta testing)
- Screen sharing provided concrete examples
- Multiple pain points identified for further investigation

**With proper follow-up research, these findings could inform high-impact product improvements.**

---

## Appendix: Quality Review Checklist

Use this checklist for future research:

### Before Research
- [ ] Research plan created (goals, hypothesis, methodology)
- [ ] Screener questions defined
- [ ] Target sample size determined (5-8 for qualitative)
- [ ] CRA process documented
- [ ] Recording consent process ready
- [ ] Interview script reviewed for leading questions

### During Research
- [ ] CRA signed before session starts
- [ ] Recording consent obtained and documented
- [ ] Open-ended questions used
- [ ] Facilitator avoided confirming/denying issues during discovery
- [ ] Screen sharing or behavioral observation used
- [ ] Participant anonymization maintained

### After Research
- [ ] Session notes document timestamps and quotes
- [ ] Findings backed by evidence (no hallucination)
- [ ] Confidence levels indicated based on sample size
- [ ] Limitations acknowledged
- [ ] Follow-up research needs identified
- [ ] Findings filed in repository with tags

---

_Research Quality Review conducted by Autodesk Research Assistant Agent_
_Purpose: Ensure research meets Autodesk Research Ops standards and findings are appropriately scoped_
